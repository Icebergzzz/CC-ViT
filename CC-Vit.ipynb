{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from my_dataset import MyDataSet\n",
    "from vit_model_dropkey import vit_base_patch32_224 as create_model\n",
    "#from utils import read_split_data, train_one_epoch, evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def main():\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using {} device.\".format(device))\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "data_transform = {\n",
    "    \"train\": transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                    #transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]),\n",
    "    \"val\": transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])}\n",
    "\n",
    "transform1 = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    #transforms.RandomHorizontalFlip(),\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))  # get data root path\n",
    "image_path = os.path.join(data_root, \"data_set\", \"flower_data\")  # flower data set path\n",
    "assert os.path.exists(image_path), \"{} path does not exist.\".format(image_path)\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"train\"),\n",
    "                                        transform=transform1)\n",
    "train_num = len(train_dataset)\n",
    "\n",
    "# {'daisy':0, 'dandelion':1, 'roses':2, 'sunflower':3, 'tulips':4}\n",
    "flower_list = train_dataset.class_to_idx\n",
    "cla_dict = dict((val, key) for key, val in flower_list.items())\n",
    "# write dict into json file\n",
    "json_str = json.dumps(cla_dict, indent=4)\n",
    "with open('class_indices.json', 'w') as json_file:\n",
    "    json_file.write(json_str)\n",
    "\n",
    "nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
    "print('Using {} dataloader workers every process'.format(nw))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            batch_size=batch_size, shuffle=True,\n",
    "                                            num_workers=0)\n",
    "\n",
    "validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, \"val\"),\n",
    "                                        transform=transform2)\n",
    "val_num = len(validate_dataset)\n",
    "validate_loader = torch.utils.data.DataLoader(validate_dataset,\n",
    "                                                batch_size=batch_size, shuffle=False,\n",
    "                                                num_workers=0)\n",
    "\n",
    "print(\"using {} images for training, {} images for validation.\".format(train_num,\n",
    "                                                                        val_num))\n",
    "print('训练集图像数量', len(train_dataset))\n",
    "print('类别个数', len(train_dataset.classes))\n",
    "print('各类别名称', train_dataset.classes)\n",
    "print('测试集图像数量', len(validate_dataset))\n",
    "print('类别个数', len(validate_dataset.classes))\n",
    "print('各类别名称', validate_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各类别名称\n",
    "class_names = train_dataset.classes\n",
    "n_class = len(class_names)\n",
    "# 映射关系：类别 到 索引号\n",
    "train_dataset.class_to_idx\n",
    "# 映射关系：索引号 到 类别\n",
    "idx_to_labels = {y:x for x,y in train_dataset.class_to_idx.items()}\n",
    "print(idx_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存 idx_to_labels 到 txt 文件\n",
    "with open('idx_to_labels.txt', 'w') as f:\n",
    "    for key, value in idx_to_labels.items():\n",
    "        f.write(f'{key}: {value}\\n')\n",
    "\n",
    "# 保存 labels_to_idx 到 txt 文件\n",
    "with open('labels_to_idx.txt', 'w') as f:\n",
    "    for key, value in train_dataset.class_to_idx.items():\n",
    "        f.write(f'{key}: {value}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(num_classes=n_class) #, has_logits=False\n",
    "\n",
    "weights = \"./vit_base_patch32_224.pth\" #./shufflenetv2_x1-5666bf0f80.pth\n",
    "\n",
    "if weights:\n",
    "    assert os.path.exists(weights), \"Weights file: '{}' does not exist.\".format(weights)\n",
    "    \n",
    "    # 加载权重字典\n",
    "    weights_dict = torch.load(weights, map_location=device)\n",
    "    \n",
    "    # 根据模型的结构删除不需要的权重\n",
    "    if hasattr(model, 'has_logits') and model.has_logits:\n",
    "        del_keys = ['head.weight', 'head.bias']\n",
    "    else:\n",
    "        del_keys = ['pre_logits.fc.weight', 'pre_logits.fc.bias', 'head.weight', 'head.bias']\n",
    "    \n",
    "    # 删除指定的权重\n",
    "    for k in del_keys:\n",
    "        if k in weights_dict:\n",
    "            del weights_dict[k]\n",
    "    \n",
    "    # 加载权重到模型中，并输出加载的状态\n",
    "    missing_keys, unexpected_keys = model.load_state_dict(weights_dict, strict=False)\n",
    "    if missing_keys:\n",
    "        print(f\"Warning: Missing keys in loaded weights: {missing_keys}\")\n",
    "    if unexpected_keys:\n",
    "        print(f\"Warning: Unexpected keys in loaded weights: {unexpected_keys}\")\n",
    "\n",
    "    print(\"Loaded weights successfully.\")\n",
    "else:\n",
    "    print(\"No weights file specified.\")\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# construct an optimizer\n",
    "#params = [p for p in model.parameters() if p.requires_grad]\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-3)  #, momentum=0.9\n",
    "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)   # 设定优优化器更新的时刻表\n",
    "\n",
    "\n",
    "#定义 warm-up 阶段的学习率调整函数\n",
    "def warmup_lr_lambda(current_step):\n",
    "    if current_step < warmup_steps:\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    return 1.0\n",
    "\n",
    "#设置 warm-up 步数\n",
    "warmup_steps = 5  # 根据需要调整\n",
    "\n",
    "#构建优化器\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1*batch_size/256, weight_decay=5e-4, momentum=0.9) #, momentum=0.9\n",
    "\n",
    "#使用 LambdaLR 实现 warm-up\n",
    "scheduler_warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lr_lambda)\n",
    "\n",
    "#设置主学习率调度器\n",
    "#scheduler_main = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "EPOCHS = 200\n",
    "lrf = 0.01\n",
    "lf = lambda x: ((1 + math.cos(x * math.pi / EPOCHS)) / 2) * (1 - lrf) + lrf  # cosine\n",
    "scheduler_main = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_batch(images, labels):\n",
    "    '''\n",
    "    运行一个 batch 的训练，返回当前 batch 的训练日志\n",
    "    '''\n",
    "    \n",
    "    # 获得一个 batch 的数据和标注\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    outputs = model(images) # 输入模型，执行前向预测\n",
    "    loss = loss_function(outputs, labels) # 计算当前 batch 中，每个样本的平均交叉熵损失函数值\n",
    "    \n",
    "    # 优化更新权重\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 获取当前 batch 的标签类别和预测类别\n",
    "    _, preds = torch.max(outputs, 1) # 获得当前 batch 所有图像的预测类别\n",
    "    preds = preds.cpu().numpy()\n",
    "    loss = loss.detach().cpu().numpy()\n",
    "    outputs = outputs.detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    \n",
    "    log_train = {}\n",
    "    log_train['epoch'] = epoch\n",
    "    log_train['batch'] = batch_idx\n",
    "    # 计算分类评估指标\n",
    "    log_train['train_loss'] = loss\n",
    "    log_train['train_accuracy'] = accuracy_score(labels, preds)\n",
    "    # log_train['train_precision'] = precision_score(labels, preds, average='macro')\n",
    "    # log_train['train_recall'] = recall_score(labels, preds, average='macro')\n",
    "    # log_train['train_f1-score'] = f1_score(labels, preds, average='macro')\n",
    "    \n",
    "    return log_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_testset():\n",
    "    '''\n",
    "    在整个测试集上评估，返回分类评估指标日志\n",
    "    '''\n",
    "\n",
    "    loss_list = []\n",
    "    labels_list = []\n",
    "    preds_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in validate_loader: # 生成一个 batch 的数据和标注\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images) # 输入模型，执行前向预测\n",
    "\n",
    "            # 获取整个测试集的标签类别和预测类别\n",
    "            _, preds = torch.max(outputs, 1) # 获得当前 batch 所有图像的预测类别\n",
    "            preds = preds.cpu().numpy()\n",
    "            loss = loss_function(outputs, labels) # 由 logit，计算当前 batch 中，每个样本的平均交叉熵损失函数值\n",
    "            loss = loss.detach().cpu().numpy()\n",
    "            outputs = outputs.detach().cpu().numpy()\n",
    "            labels = labels.detach().cpu().numpy()\n",
    "\n",
    "            loss_list.append(loss)\n",
    "            labels_list.extend(labels)\n",
    "            preds_list.extend(preds)\n",
    "        \n",
    "    log_test = {}\n",
    "    log_test['epoch'] = epoch\n",
    "    \n",
    "    # 计算分类评估指标\n",
    "    log_test['test_loss'] = np.mean(loss_list)\n",
    "    log_test['test_accuracy'] = accuracy_score(labels_list, preds_list)\n",
    "    log_test['test_precision'] = precision_score(labels_list, preds_list, average='macro')\n",
    "    log_test['test_recall'] = recall_score(labels_list, preds_list, average='macro')\n",
    "    log_test['test_f1-score'] = f1_score(labels_list, preds_list, average='macro')\n",
    "    \n",
    "    return log_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "batch_idx = 0\n",
    "best_test_accuracy = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练日志-训练集\n",
    "df_train_log = pd.DataFrame()\n",
    "log_train = {}\n",
    "log_train['epoch'] = 0\n",
    "log_train['batch'] = 0\n",
    "images, labels = next(iter(train_loader))\n",
    "log_train.update(train_one_batch(images, labels))\n",
    "df_train_log = df_train_log._append(log_train, ignore_index=True)\n",
    "df_train_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练日志-测试集\n",
    "df_test_log = pd.DataFrame()\n",
    "log_test = {}\n",
    "log_test['epoch'] = 0\n",
    "log_test.update(evaluate_testset())\n",
    "df_test_log = df_test_log._append(log_test, ignore_index=True)\n",
    "df_test_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, EPOCHS+1):\n",
    "    \n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    \n",
    "    ## 训练阶段\n",
    "    model.train()\n",
    "    for images, labels in tqdm(train_loader): # 获得一个 batch 的数据和标注\n",
    "        batch_idx += 1\n",
    "        log_train = train_one_batch(images, labels)\n",
    "        df_train_log = df_train_log._append(log_train, ignore_index=True)\n",
    "        \n",
    "        \n",
    "        # 更新 warm-up 调度器\n",
    "        scheduler_warmup.step()     \n",
    "    #lr_scheduler.step()\n",
    "\n",
    "    ## 测试阶段\n",
    "    model.eval()\n",
    "    log_test = evaluate_testset()\n",
    "    df_test_log = df_test_log._append(log_test, ignore_index=True)\n",
    "    \n",
    "    \n",
    "    # 检查是否存在 'checkpoint' 文件夹，不存在则创建\n",
    "    checkpoint_dir = 'checkpoint'\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    # 保存最新的最佳模型文件\n",
    "    if log_test['test_accuracy'] > best_test_accuracy: \n",
    "        # 删除旧的最佳模型文件(如有)\n",
    "        old_best_checkpoint_path = 'checkpoint/best-{:.3f}.pth'.format(best_test_accuracy)\n",
    "        if os.path.exists(old_best_checkpoint_path):\n",
    "            os.remove(old_best_checkpoint_path)\n",
    "        # 保存新的最佳模型文件\n",
    "        best_test_accuracy = log_test['test_accuracy']\n",
    "        new_best_checkpoint_path = 'checkpoint/best-{:.3f}.pth'.format(log_test['test_accuracy'])\n",
    "        torch.save(model, new_best_checkpoint_path)\n",
    "        print('保存新的最佳模型', 'checkpoint/best-{:.3f}.pth'.format(best_test_accuracy))\n",
    "        # best_test_accuracy = log_test['test_accuracy']\n",
    "\n",
    "    # 在每个 epoch 结束时更新主调度器\n",
    "    scheduler_main.step()        \n",
    "\n",
    "df_train_log.to_csv('训练日志-训练集.csv', index=False)\n",
    "df_test_log.to_csv('训练日志-测试集.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
